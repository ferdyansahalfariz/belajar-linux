# kafka connect

pada kesempatan kali ini, saya akan mencoba mempelajari dan sedikit praktek tentang kafka connect yang mana berfungsi untuk menyambungkan kafka ke database ataupun platform lainnya.

## Definisi Kafka Connect
Kafka Connect adalah framework yang disediakan oleh Apache Kafka untuk streaming data antara Kafka dan sistem eksternal dengan cara yang scalable dan fault-tolerant. Kafka Connect memudahkan integrasi antara Kafka dan berbagai sistem data seperti databases, message queues, dan file systems tanpa perlu menulis banyak kode custom.

### Fitur utama dari Kafka Connect meliputi:

1. Scalability: Kafka Connect dapat di-scale baik secara vertikal maupun horizontal untuk menangani beban kerja yang besar.
2. Fault Tolerance: Kafka Connect memastikan bahwa data tidak hilang selama proses transfer dengan menggunakan mekanisme yang andal.
3. Configurability: Konfigurasi yang sederhana melalui file properties atau REST API.
4. Integrasi yang Luas: Mendukung berbagai konektor untuk sistem data populer.

## Connector
Connector adalah plugin yang digunakan oleh Kafka Connect untuk membaca data dari atau menulis data ke sistem eksternal. Ada dua jenis connector:

1. Source Connector: Membaca data dari sistem eksternal dan menulisnya ke Kafka topics.
2. Sink Connector: Membaca data dari Kafka topics dan menulisnya ke sistem eksternal.

Connector berfungsi sebagai jembatan antara Kafka dan sistem data lainnya, memungkinkan aliran data yang efisien dan mudah.

## Source Connector
Source Connector adalah jenis connector yang digunakan untuk mengambil data dari sistem eksternal (seperti database, file system, atau message queue) dan mengirimkan data tersebut ke Kafka topics. Source Connector terus-menerus memantau sistem sumber untuk data baru dan meneruskannya ke Kafka.

### Contoh penggunaan Source Connector:

Membaca data perubahan dari database menggunakan JDBC Source Connector dan mengirimkan perubahan tersebut ke Kafka.
Mengambil data log dari file dan menulisnya ke Kafka untuk analisis lebih lanjut.

## Sink Connector
Sink Connector adalah jenis connector yang digunakan untuk mengambil data dari Kafka topics dan menuliskannya ke sistem eksternal (seperti database, file system, atau message queue). Sink Connector memungkinkan data yang telah diproses atau disimpan di Kafka untuk dikonsumsi oleh sistem lain.

### Contoh penggunaan Sink Connector:

Menulis data dari Kafka topics ke database untuk pelaporan atau analisis lebih lanjut menggunakan JDBC Sink Connector.
Menyimpan data dari Kafka ke file system atau cloud storage untuk pengarsipan.

## Kesimpulan
Kafka Connect: Framework untuk integrasi data antara Kafka dan sistem eksternal.
Connector: Plugin untuk membaca atau menulis data antara Kafka dan sistem eksternal.
Source Connector: Membaca data dari sistem eksternal ke Kafka.
Sink Connector: Menulis data dari Kafka ke sistem eksternal.
Kafka Connect, bersama dengan Source dan Sink Connectors, memberikan cara yang sederhana dan andal untuk mengintegrasikan Kafka dengan berbagai sistem data yang ada, memungkinkan aliran data yang efisien dan realtime antara berbagai komponen sistem.

# Praktek
Pada praktek kali ini, saya akan mencoba untuk membuat sink connector yang berfungsi untuk memasukan data dari topic matches pada topik ksql sebelumnya, kedalam database postgresql.

1. langkah pertama yaitu menginstall postgresql, lakukan dengan perintah :

sudo yum install postgresql13-server postgresql13
sudo yum install pgxnclient
sudo /usr/pgsql-13/bin/postgresql-13-setup initdb
sudo systemctl start postgresql
sudo systemctl status postgresql

perintah diatas akan melakukan install postgresql 13, lalu memulai inisialisasi databasenya dan memulai postgresql di server melalui systemd, pastikan status dari service postgresql sudah active.

2. masuk ke user postgres dengan su postgres lalu mulai masuk ke cli postgresql dengan perintah psql.

3. Setelah masuk, buat database baru kemudian buat juga user serta passwordnya

CREATE DATABASE postgres_sink;
CREATE USER ferdy WITH PASSWORD 'ferdy';
GRANT ALL PRIVILEGES ON DATABASE postgres_sink TO ferdy;

4. buat file konfigurasi berbentuk json dengan isi:

{
  "name": "postgres-sink-connector",
  "config": {
    "connector.class": "io.confluent.connect.jdbc.JdbcSinkConnector",
    "tasks.max": "2",
    "key.converter": "io.confluent.connect.avro.AvroConverter",
    "value.converter": "io.confluent.connect.avro.AvroConverter",
    "transforms": "InsertField",
    "topics": "matches",
    "transforms.InsertField.type": "org.apache.kafka.connect.transforms.InsertField$Value",
    "transforms.InsertField.offset.field": "kafka_offset",
    "transforms.InsertField.timestamp.field": "kafka_timestamp",
    "connection.url": "jdbc:postgresql://10.100.13.24:5432/postgres_sink",
    "connection.user": "ferdy",
    "connection.password": "ferdy",
    "dialect.name": "PostgreSqlDatabaseDialect",
    "insert.mode": "insert",
    "table.name.format": "public.matches",
    "pk.mode": "none",
    "key.converter.schema.registry.url": "http://master.k8s.alldataint.com:8081",
    "value.converter.schema.registry.url": "http://master.k8s.alldataint.com:8081",
    "auto.create": "true",
    "auto.evolve": "true"
  }
}

5. Buka https://www.confluent.io/hub/ untuk mengunduh connector jdbc source and sink lalu extract pada folder yang digunakan dalam properties di plugin.path pada file connect-distributed.properties. dalam hal ini saya meletakannya di /usr/share/confluent-hub-components dan menambahkan pathnya ke plugin.path dengan menambahkan koma di path lainnya.
  
6. masuk ke c3 pada bagian connect lalu buat connector baru yaitu jdbc sink lalu import file configurasi yang sudah dibuat tadi, lalu launch seperti gambar dibawah.

![image](https://github.com/ferdyansahalfariz/belajar-linux/assets/96871156/1bc19a54-bc96-4deb-bd73-8d66a53e22a6)

Setelah itu maka connector sudah terhubung seperti berikut:

![image](https://github.com/ferdyansahalfariz/belajar-linux/assets/96871156/61ebf95b-5d23-448f-b79c-68c58db3db49)

lakukan pemeriksaan di database postgresql seperti dibawah dengan menggunakan dbeaver seperti berikut yang menunjukan data dari topic sudah berhasil dimasukan ke database.

![image](https://github.com/ferdyansahalfariz/belajar-linux/assets/96871156/60f700c9-0f27-4846-87b4-26611b1802b2)

# kafka connect

pada kesempatan kali ini, saya akan mencoba mempelajari dan sedikit praktek tentang kafka connect yang mana berfungsi untuk menyambungkan kafka ke database ataupun platform lainnya.

## Definisi Kafka Connect
Kafka Connect adalah framework yang disediakan oleh Apache Kafka untuk streaming data antara Kafka dan sistem eksternal dengan cara yang scalable dan fault-tolerant. Kafka Connect memudahkan integrasi antara Kafka dan berbagai sistem data seperti databases, message queues, dan file systemd tanpa perlu menulis banyak kode custom.

### Fitur utama dari Kafka Connect meliputi:

1. Scalability: Kafka Connect dapat di-scale baik secara vertikal maupun horizontal untuk menangani beban kerja yang besar.
2. Fault Tolerance: Kafka Connect memastikan bahwa data tidak hilang selama proses transfer dengan menggunakan mekanisme yang andal.
3. Configurability: Konfigurasi yang sederhana melalui file properties atau REST API.
4. Integrasi yang Luas: Mendukung berbagai konektor untuk sistem data populer.

## Connector
Connector adalah plugin yang digunakan oleh Kafka Connect untuk membaca data dari atau menulis data ke sistem eksternal. Ada dua jenis connector:

1. Source Connector: Membaca data dari sistem eksternal dan menulisnya ke Kafka topics.
2. Sink Connector: Membaca data dari Kafka topics dan menulisnya ke sistem eksternal.

Connector berfungsi sebagai jembatan antara Kafka dan sistem data lainnya, memungkinkan aliran data yang efisien dan mudah.

## Source Connector
Source Connector adalah jenis connector yang digunakan untuk mengambil data dari sistem eksternal (seperti database, file system, atau message queue) dan mengirimkan data tersebut ke Kafka topics. Source Connector terus-menerus memantau sistem sumber untuk data baru dan meneruskannya ke Kafka.

### Contoh penggunaan Source Connector:

Membaca data perubahan dari database menggunakan JDBC Source Connector dan mengirimkan perubahan tersebut ke Kafka.
Mengambil data log dari file dan menulisnya ke Kafka untuk analisis lebih lanjut.

## Sink Connector
Sink Connector adalah jenis connector yang digunakan untuk mengambil data dari Kafka topics dan menuliskannya ke sistem eksternal (seperti database, file system, atau message queue). Sink Connector memungkinkan data yang telah diproses atau disimpan di Kafka untuk dikonsumsi oleh sistem lain.

### Contoh penggunaan Sink Connector:

Menulis data dari Kafka topics ke database untuk pelaporan atau analisis lebih lanjut menggunakan JDBC Sink Connector.
Menyimpan data dari Kafka ke file system atau cloud storage untuk pengarsipan.

## Kesimpulan
Kafka Connect: Framework untuk integrasi data antara Kafka dan sistem eksternal.
Connector: Plugin untuk membaca atau menulis data antara Kafka dan sistem eksternal.
Source Connector: Membaca data dari sistem eksternal ke Kafka.
Sink Connector: Menulis data dari Kafka ke sistem eksternal.
Kafka Connect, bersama dengan Source dan Sink Connectors, memberikan cara yang sederhana dan andal untuk mengintegrasikan Kafka dengan berbagai sistem data yang ada, memungkinkan aliran data yang efisien dan realtime antara berbagai komponen sistem.

# Praktek
Pada praktek kali ini, saya akan mencoba untuk membuat sink connector yang berfungsi untuk memasukan data dari topic ```matches``` pada topik ksql sebelumnya, kedalam database postgresql.

1. langkah pertama yaitu menginstall postgresql, lakukan dengan perintah :

```
sudo yum install postgresql13-server postgresql13
sudo yum install pgxnclient
sudo /usr/pgsql-13/bin/postgresql-13-setup initdb
sudo systemctl start postgresql
sudo systemctl status postgresql
```

perintah diatas akan melakukan install postgresql 13, lalu memulai inisialisasi databasenya dan memulai postgresql di server melalui systemd, pastikan status dari service postgresql sudah active.

2. masuk ke user postgres dengan su postgres lalu mulai masuk ke cli postgresql dengan perintah psql.

3. Setelah masuk, buat database baru kemudian buat juga user serta passwordnya. tidak lupa juga dengan memberikan akses penuh databse untuk user.

```
CREATE DATABASE postgres_sink;
CREATE USER ferdy WITH PASSWORD 'ferdy';
GRANT ALL PRIVILEGES ON DATABASE postgres_sink TO ferdy;
```

4. buat file konfigurasi berbentuk json dengan isi:

```
{
  "name": "postgres-sink-connector",
  "config": {
    "connector.class": "io.confluent.connect.jdbc.JdbcSinkConnector",
    "tasks.max": "2",
    "key.converter": "io.confluent.connect.avro.AvroConverter",
    "value.converter": "io.confluent.connect.avro.AvroConverter",
    "transforms": "InsertField",
    "topics": "matches",
    "transforms.InsertField.type": "org.apache.kafka.connect.transforms.InsertField$Value",
    "transforms.InsertField.offset.field": "kafka_offset",
    "transforms.InsertField.timestamp.field": "kafka_timestamp",
    "connection.url": "jdbc:postgresql://10.100.13.24:5432/postgres_sink",
    "connection.user": "ferdy",
    "connection.password": "ferdy",
    "dialect.name": "PostgreSqlDatabaseDialect",
    "insert.mode": "insert",
    "table.name.format": "public.matches",
    "pk.mode": "none",
    "key.converter.schema.registry.url": "http://master.k8s.alldataint.com:8081",
    "value.converter.schema.registry.url": "http://master.k8s.alldataint.com:8081",
    "auto.create": "true",
    "auto.evolve": "true"
  }
}
```

5. Buka ```https://www.confluent.io/hub/``` untuk mengunduh connector jdbc source and sink lalu extract pada folder yang digunakan dalam properties di ```plugin.path``` pada file ```connect-distributed.properties```. dalam hal ini saya meletakannya di ```/usr/share/confluent-hub-components``` dan menambahkan pathnya ke plugin.path dengan menambahkan koma di path lainnya.
  
6. masuk ke c3 pada bagian connect lalu buat connector baru yaitu jdbc sink lalu import file configurasi yang sudah dibuat tadi, lalu launch seperti gambar dibawah.

![image](https://github.com/ferdyansahalfariz/belajar-linux/assets/96871156/1bc19a54-bc96-4deb-bd73-8d66a53e22a6)

Setelah itu maka connector sudah terhubung seperti berikut:

![image](https://github.com/ferdyansahalfariz/belajar-linux/assets/96871156/61ebf95b-5d23-448f-b79c-68c58db3db49)

lakukan pemeriksaan di database postgresql seperti dibawah dengan menggunakan dbeaver seperti berikut yang menunjukan data dari topic sudah berhasil dimasukan ke database.

![image](https://github.com/ferdyansahalfariz/belajar-linux/assets/96871156/60f700c9-0f27-4846-87b4-26611b1802b2)

# 20 Juni 2024

Saat ini tugas saya melanjutkan untuk mengirim 1 juta data ke topic. setelah sebelumnya dilakukan dengan menggunakan ksql via query, sekarang saya langsung melakukannya direct to topic dengan program java. pengiriman 1 juta data terjadi dengan lancar dan cepat dan sebelum pengisian data, dilakukan sink terlebih dahulu ke database postgresql, berikut adalah configurasinya dan hasilnya:

```
{
  "name": "postgres-testsatujuta-sink-connector",
  "config": {
    "value.converter.schema.registry.url": "http://master.k8s.alldataint.com:8081",
    "key.converter.schema.registry.url": "http://master.k8s.alldataint.com:8081",
    "name": "postgres-testsatujuta-sink-connector",
    "connector.class": "io.confluent.connect.jdbc.JdbcSinkConnector",
    "tasks.max": "2",
    "key.converter": "io.confluent.connect.avro.AvroConverter",
    "value.converter": "io.confluent.connect.avro.AvroConverter",
    "transforms": "InsertField",
    "topics": "testsejuta",
    "transforms.InsertField.type": "org.apache.kafka.connect.transforms.InsertField$Value",
    "transforms.InsertField.offset.field": "kafka_offset",
    "transforms.InsertField.partition.field": "kafka_partition",
    "transforms.InsertField.timestamp.field": "kafka_timestamp",
    "connection.url": "jdbc:postgresql://10.100.13.24:5432/testsejuta",
    "connection.user": "ferdy",
    "connection.password": "ferdy",
    "dialect.name": "PostgreSqlDatabaseDialect",
    "insert.mode": "insert",
    "batch.size": "5000",
    "table.name.format": "public.matchsatujuta",
    "pk.mode": "none",
    "auto.create": "true",
    "auto.evolve": "true"
  }
}
```

berikut adalah isi program javanya:

```
package com.example;

import org.apache.avro.Schema;
import org.apache.avro.generic.GenericData;
import org.apache.avro.generic.GenericRecord;
import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.Producer;
import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.apache.kafka.clients.producer.RecordMetadata;
import io.confluent.kafka.serializers.KafkaAvroSerializer;
import java.util.Properties;
import java.io.FileInputStream;
import java.io.IOException;
import java.util.concurrent.ExecutionException;

public class ProducerMatch {
    public static void main(String[] args) {
        String topic = "testsejuta";

        Properties properties = new Properties();
	try (FileInputStream producerConfig = new FileInputStream("/testsejuta/src/main/resources/sejutaConfig.properties")) {
            properties.load(producerConfig);
        } catch (IOException e) {
            System.err.println("Error loading configuration: " + e.getMessage());
            return;
        }

        try (KafkaProducer<String, GenericRecord> producer = new KafkaProducer<>(properties)) {
            Schema schema = new Match().getSchema();

            for (int i = 1; i <= 1000000; i++) {
                String matchId = "match" + (i % 5 + 1);
                String team = i % 2 == 0 ? "barca" : "madrid";
                int score = i % 3;

                GenericRecord avroRecord = new GenericData.Record(schema);
                avroRecord.put("matchId", matchId);
                avroRecord.put("team", team);
                avroRecord.put("score", score);

                ProducerRecord<String, GenericRecord> record = new ProducerRecord<>(topic, avroRecord);
                producer.send(record);
            }
            System.out.println("Sent 1M records to Kafka");
        } catch (Exception e) {
            e.printStackTrace();
        }
    }
}
```

Sebelum pengiriman data, saya juga membuat tabel terlebih dahulu dengan atribut tambahan dari database postgresql dengan perintah :

```
CREATE TABLE public.matchsatujuta (
	landing_timestamp TIMESTAMP DEFAULT current_timestamp
);
```

landing timestamp akan mencatat kapan data tiap row masuk ke dalam database. Dan berikut adalah hasilnya setelah sink ke database posgresql:

![image](https://github.com/ferdyansahalfariz/belajar-linux/assets/96871156/6cecf602-30ac-4c35-9f9e-b5fe994ebbf6)

![image](https://github.com/ferdyansahalfariz/belajar-linux/assets/96871156/33a03c06-d444-4dac-acdf-675a5befb097)

terlihat pada data awal offset 0, data masuk ke kafka pada menit 42 detik ke 3 dan selesai di detik ke 7 yang berarti memakan waktu 4 detik, namun data kemudian mulai di sink pada menit 42 detik 4 dan selesai 8 detik kemudian.

Berikut adalah hasil saat seluruh row di dalam database dihitung menggunakan ```SELECT COUNT(*) FROM matchsatujuta;``` untuk melihat apakah ada data loss:

![image](https://github.com/ferdyansahalfariz/belajar-linux/assets/96871156/cb339917-33a1-4686-b12a-10ee22adfdc8)

Kemudian saya akan mencoba meihat apakah terdapat data duplikasi yang terjadi pada database, hal ini dapat dilakukan dengan melihat data kombinasi dari ```kafka_partition``` dan ```kafka_offset``` yang mana bersifat unik dan incremental, jadi jika ada data yang memiliki value yang sama pada keduanya, maka data tersebut duplikat. Berikut querynya:

```
SELECT kafka_partition, kafka_offset, COUNT(*)
FROM matchsatujuta
GROUP BY kafka_partition, kafka_offset
HAVING COUNT(*) > 1;
```

Query tersebut akan mengambil data kafka partition dan offset yang dikelompokan berdasarkan hasil yang sama lalu dihitung juga jumlahnya. jika hasil query menunjukan hasil yang kosong, artinya tidak ada data yang terduplikasi dikarenakan setiap kombinasinya hanya memiliki 1 row dan berikut hasilnya:

![image](https://github.com/ferdyansahalfariz/belajar-linux/assets/96871156/13410995-241e-402d-b371-8bcd21d19f80)

Untuk make sure, lakukan penghitungan jumlah row berdasarkan kombinasi dari partisi dan offset setiap data sebagai unique_pairs. kali ini akan menggunakan query DISTINCT.Query DISTINCT dalam SQL digunakan untuk menghilangkan duplikasi dalam hasil query, sehingga hanya menampilkan baris-baris yang unik. Tujuannya adalah untuk memastikan bahwa setiap nilai yang diambil dari kolom atau kombinasi kolom adalah unik, tanpa ada duplikasi. dengan menggunakan DISTINCT, lalu kombinasikan dengan COUNT maka data yang unik berdasarkan kafka pertisi dan offset pada tiap row dapat dihitung dengan query :

```
SELECT COUNT(*)
FROM (
  SELECT DISTINCT kafka_partition, kafka_offset
  FROM matchsatujuta
) AS unique_pairs;
```

query tersebut akan menghasilkan jumlah row yang memiliki kombinasi partisi dan offset yang unik, jika terdapat duplikasi partisi dan offset pada lebih dari 1 row, maka row tersebut tidak akan dihitung, berikut adalah hasilnya:

![image](https://github.com/ferdyansahalfariz/belajar-linux/assets/96871156/2d6d379e-00f2-44f9-86ff-d24c695a3618)


## Kirim 100 juta data

Selanjutnya dengan proses yang sama, dilakukan pengiriman data namun dengan jumlah sebanyak 100 juta data. untuk melakukan optimalisasinya, maka perlu dilakukannya tuning yang dapat dilihat pada referensi berikut:

* https://docs.confluent.io/cloud/current/client-apps/optimizing/index.html
* https://docs.confluent.io/cloud/current/connectors/cc-postgresql-sink.html#consumer-configuration

dari kedua referensi tersebut, saya mendapatkan informasi jika ingin memaksimalkan pengiriman ke connector untuk keperluan sink, maka terdapat beberapa faktor yang berpengaruh pada konfigurasi yang ada yaitu dengan memaksimalkan ```batch.size``` menjadi maks ```5000```, serta memperbanyak jumlah ```tasks.max``` dengan tujuan untuk menjalankan tugas sink secara paralel, disini saya meningkatkan ```tasks.max``` nya menjadi ```3```, lalu ```5``` dan naik ke ```10``` disaat proses sink terjadi di 45 juta data dikarenakan memakan waktu yang lama (```select COUNT(*) from matchseratusjuta;```). berikut adalah konfigurasinya:

```
{
  "name": "postgres-testseratusjuta-sink-connector",
  "config": {
    "value.converter.schema.registry.url": "http://master.k8s.alldataint.com:8081",
    "key.converter.schema.registry.url": "http://master.k8s.alldataint.com:8081",
    "name": "postgres-testseratusjuta-sink-connector",
    "connector.class": "io.confluent.connect.jdbc.JdbcSinkConnector",
    **"tasks.max": "10",**
    "key.converter": "io.confluent.connect.avro.AvroConverter",
    "value.converter": "io.confluent.connect.avro.AvroConverter",
    "transforms": "InsertField",
    "topics": "testseratusjuta",
    "transforms.InsertField.type": "org.apache.kafka.connect.transforms.InsertField$Value",
    "transforms.InsertField.offset.field": "kafka_offset",
    "transforms.InsertField.partition.field": "kafka_partition",
    "transforms.InsertField.timestamp.field": "kafka_timestamp",
    "connection.url": "jdbc:postgresql://10.100.13.24:5432/testseratusjuta",
    "connection.user": "ferdy",
    "connection.password": "*****",
    "dialect.name": "PostgreSqlDatabaseDialect",
    "insert.mode": "insert",
    **"batch.size": "5000",**
    "table.name.format": "public.matchseratusjuta",
    "pk.mode": "none",
    "auto.create": "true",
    "auto.evolve": "true"
  }
}
```

barikut adalah isi awal data di offset 0:

![image](https://github.com/ferdyansahalfariz/belajar-linux/assets/96871156/20dd90f5-fdbb-41d4-9603-55d87baf742c)

![image](https://github.com/ferdyansahalfariz/belajar-linux/assets/96871156/038cb3e6-8f5e-43a2-8750-889f2603b1a6)

![image](https://github.com/ferdyansahalfariz/belajar-linux/assets/96871156/16f04c06-e1a5-4bbf-ad5d-9651004b3a57)

Pengisian data di kafka tercepat terjadi di partisi 1 dan dimulai pada menit 7 detik 59 dan mulai di sink ke database pada menit 8 yang memakan waktu 1 detik.
## 
Berikut adalah hasil setelah seluruh 100 juta data berhasil di sink, terlihat pada data di offset terakhir, kafka menerimanya pada jam 14:11:40, dan data terakhir dalam partisi 0 berhasil di sink ke database pada pukul 16:04:23 yang berarti memakan waktu 1 jam 52 menit 43 detik.

![image](https://github.com/ferdyansahalfariz/belajar-linux/assets/96871156/55f40eb8-269c-4899-8dbc-488720df8d1e)

![image](https://github.com/ferdyansahalfariz/belajar-linux/assets/96871156/e4ccb13f-a794-4c52-ba16-6f5babde9579)

![image](https://github.com/ferdyansahalfariz/belajar-linux/assets/96871156/33aabefd-1403-4189-baa5-40a1b159950c)


Berikut adalah hasil saat seluruh row di dalam database dihitung menggunakan ```SELECT COUNT(*) FROM matchseratusjuta;``` untuk melihat apakah ada data loss:

![image](https://github.com/ferdyansahalfariz/belajar-linux/assets/96871156/8268c9e4-fe23-48b3-9b0a-39947ef0ffed)

hasil pengecekan data duplicate dengan query:

```
SELECT kafka_partition, kafka_offset, COUNT(*)
FROM matchseratusjuta
GROUP BY kafka_partition, kafka_offset
HAVING COUNT(*) > 1;
```

![image](https://github.com/ferdyansahalfariz/belajar-linux/assets/96871156/9cec6151-7930-4830-af01-e844ccdfaa43)

Terakhir yaitu menghitung hasil kombinasi yang unik dari kafka partisi dan offset sebagai unique_pairs seperti pada data 1 juta diatas dengan hasil berikut:

![image](https://github.com/ferdyansahalfariz/belajar-linux/assets/96871156/f40ccfdb-4f6c-4073-9096-6f1f7a510eee)

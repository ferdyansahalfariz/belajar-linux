# Schema Registry

![image](https://github.com/ferdyansahalfariz/belajar-linux/assets/96871156/cd9bbf94-4b0a-4d41-9314-7ff531e2ed28)

Schema Registry adalah komponen penting dalam ekosistem Apache Kafka yang memungkinkan pengguna untuk menyimpan skema data yang digunakan untuk serialisasi dan deserialisasi pesan yang diproduksi dan dikonsumsi oleh Kafka. Berikut adalah penjelasan singkat tentang konsep Schema Registry:

Definisi Skema: Sebelum mengirimkan data ke Kafka, data harus diserialisasi menjadi format yang dapat dibaca oleh Kafka. Schema digunakan untuk menentukan struktur data yang digunakan untuk serialisasi. Contoh format skema yang umum digunakan adalah Avro, JSON, atau Protobuf.

Penyimpanan Skema: Schema Registry menyimpan semua skema yang digunakan dalam ekosistem Kafka. Ini memungkinkan aplikasi pengirim dan penerima untuk secara dinamis mengambil skema yang diperlukan untuk membaca dan menulis data.

Evolutionary Compatibility: Salah satu fitur utama dari Schema Registry adalah kemampuannya untuk mengelola evolusi skema. Ini berarti Anda dapat melakukan perubahan pada skema yang ada tanpa mempengaruhi aplikasi yang sudah ada yang menggunakan skema lama.

Validasi Skema: Schema Registry juga dapat digunakan untuk memvalidasi data yang dikirimkan ke Kafka berdasarkan skema yang terdaftar. Ini membantu memastikan bahwa data yang dikirimkan sesuai dengan format yang diharapkan.

Interoperabilitas: Schema Registry mendukung beberapa format skema yang populer, seperti Avro, JSON, dan Protobuf, sehingga memungkinkan aplikasi yang menggunakan format skema yang berbeda untuk berinteraksi dengan Kafka.

Dengan demikian, Schema Registry memberikan manfaat besar dalam manajemen skema data dalam ekosistem Kafka, memastikan konsistensi dan interoperabilitas antara aplikasi yang berbeda.

## Produce & consume Avro via CLI
untuk pemebelajaran mengenai schema registry, kali ini saya akan mulai dengan membuat producer dan consumer via CLI untuk publish maupun subscribe message berformat Avro. avro sendiri merupakan salah satu tipe message yang dapat dikirim melalui kafka dan membutuhkan validasi dari schema yang ada. karenanya, diperlukan pengecekan ke schema registry terlebih dahulu sebelum melakukan publish ataupun subscribe.

### buat schemanya dan input ke kafka
skema yang digunakan sebagai berikut:
```
{
  "type": "record",
  "name": "Ulangan",
  "fields": [
    {"name": "id", "type": "string"},
    {"name": "nilai", "type": "double"}
  ]
}
```

tahap pertama yaitu input schema yang diinginkan dengan perintah :
```
curl -X POST -H "Content-Type: application/vnd.schemaregistry.v1+json" \
     --data "{\"schema\": \"{\\\"type\\\":\\\"record\\\",\\\"name\\\":\\\"Ulangan\\\",\\\"fields\\\":[{\\\"name\\\":\\\"id\\\",\\\"type\\\":\\\"string\\\"},{\\\"name\\\":\\\"nilai\\\",\\\"type\\\":\\\"double\\\"}]}\"}" \
     http://master.k8s.alldataint.com:8081/subjects/ulangan-value/versions
```
dari perintah tersebut, data berupa skema yang diinginkan diinput kedalam service schema registry dengan port 8081. Kemudian setelah berhasil maka terdapat tanda {"id":1}[root@master /]# yang menandakan schema registry sudah berhasil diinput.

![image](https://github.com/ferdyansahalfariz/belajar-linux/assets/96871156/a9082e09-7290-4710-b0e7-c21f34e17c97)

### Produce message
selanjutnya lakukan produce message dengan perintah:
```
kafka-avro-console-producer --broker-list master.k8s.alldataint.com:9092 --topic ulangan --property schema.registry.url=http://master.k8s.alldataint.com:8081 --property value.schema.id=1
```
atau dapat langsung memasukan message dan schema yang diinginkan dengan perintah:
```
echo "{\"id\": \"2\", \"nilai\": 90.0}" | kafka-avro-console-producer --broker-list master.k8s.alldataint.com:9092 --topic ulangan --property schema.registry.url=http://master.k8s.alldataint.com:8081 --property value.schema='{"type":"record","name":"Ulangan","fields":[{"name":"id","type":"string"},{"name":"nilai","type":"double"}]}'
```
property schema registry url dan value schema id dibutuhkan untuk menerapkan schema yang akan dijadikan blueprint terhadap message yang akan dikirim. lakukan publish message yang sudah sesuai dengan skemam jika tidak maka otomatis akan gagal.

jika message tak sesuai format skema, maka akan ditolak dan muncul error berikut:

![image](https://github.com/ferdyansahalfariz/belajar-linux/assets/96871156/61f1e2bc-a17b-4e13-a27d-f543366c54ea)

### Consume message
untuk consume data avro lakukan perintah :
```
kafka-avro-console-consumer --bootstrap-server master.k8s.alldataint.com:9092 --topic ulangan --from-beginning --property schema.registry.url=http://master.k8s.alldataint.com:8081 --property value.schema.id=1
```

![image](https://github.com/ferdyansahalfariz/belajar-linux/assets/96871156/1c612ad8-a58f-4839-86f5-219d530241a6)

## Membuat program produce/consume avro dengan java
Terdapat sedikit perbedaan yang membedakan produce/consume message menggunakan avro yang mana harus membuat class yang merepresentasikan skema yang diinginkan, selain itu terdapat penerapan kafkaAvroSerializer untuk menerjemahkan message dari atau ke format avro.

Berikut adalah isi programnya:

buat file di direktori /src/main/resources sebagai properties untuk producer dengan nama producerConfig.properties dan isikan :

```
bootstrap.servers=master.k8s.alldataint.com:9092
key.serializer=io.confluent.kafka.serializers.KafkaAvroSerializer
value.serializer=io.confluent.kafka.serializers.KafkaAvroSerializer
schema.registry.url=http://master.k8s.alldataint.com:8081
```

buat juga file lain dengan nama consumerConfig.properties di direktori yang sama untuk konfigurasi properties consumer dan isikan:

```
bootstrap.servers=master.k8s.alldataint.com:9092
schema.registry.url=http://master.k8s.alldataint.com:8081
group.id=ulangan
key.deserializer=io.confluent.kafka.serializers.KafkaAvroDeserializer
value.deserializer=io.confluent.kafka.serializers.KafkaAvroDeserializer
auto.offset.reset=earliest
```

ProducerApp.java

```
package com.example;

import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.Producer;
import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.apache.kafka.clients.producer.RecordMetadata;
import org.apache.avro.generic.GenericData;
import org.apache.avro.generic.GenericRecord;
import io.confluent.kafka.serializers.KafkaAvroSerializer;

import java.util.Properties;
import java.io.FileInputStream;
import java.io.IOException;
import java.util.concurrent.ExecutionException;

public class ProducerApp {

    public static void main(String[] args) {

        Properties properties = new Properties();

	try (FileInputStream producerConfig = new FileInputStream("/avroproducer-consumer/src/main/resources/producerConfig.properties")) {
            properties.load(producerConfig);
        } catch (IOException e) {
            System.err.println("Error loading configuration: " + e.getMessage());
            return;
        }

        try (KafkaProducer<String, GenericRecord> producer = new KafkaProducer<>(properties)) {
            double nilaiAwal = 80.00;
            for (int i = 1; i <= 5; i++) {
                    // Create a Ulangan object
                    Ulangan ulangan = new Ulangan(String.valueOf(i), nilaiAwal);

                    // Create a GenericRecord based on the Ulangan object
                    GenericRecord genericRecord = new GenericData.Record(ulangan.getSchema());
                    genericRecord.put("id", ulangan.getId());
                    genericRecord.put("nilai", ulangan.getNilai());

                    // Produce the Avro record to Kafka
                    ProducerRecord<String, GenericRecord> record = new ProducerRecord<>("ulangan", Integer.toString(i), genericRecord);
                    producer.send(record, (RecordMetadata metadata, Exception exception) -> {
                if (exception == null) {
                        System.out.printf("Avro record terkirim ke partisi %d dengan offset %d%n : ", metadata.partition(), metadata.offset());
                } else {
                    exception.printStackTrace();
                }
            });
            nilaiAwal += 5;
            }
        } catch (Exception e) {
            e.printStackTrace();
        }

    }
}
```

ConsumerApp.java

```
package com.example;

import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.clients.consumer.KafkaConsumer;
import org.apache.avro.generic.GenericRecord;
import io.confluent.kafka.serializers.KafkaAvroDeserializer;

import java.time.Duration;
import java.util.Collections;
import java.util.Properties;
import java.util.List;
import java.io.FileInputStream;
import java.io.IOException;

public class ConsumerApp {

    public static void main(String[] args) {

        Properties properties = new Properties();

        try (FileInputStream consumerConfig = new FileInputStream("/avroproducer-consumer/src/main/resources/consumerConfig.properties")) {
            properties.load(consumerConfig);
        } catch (IOException e) {
            System.err.println("Error loading configuration: " + e.getMessage());
            return;
        }

        KafkaConsumer<String, GenericRecord> consumer = new KafkaConsumer<>(properties);
        consumer.subscribe(List.of("ulangan"));

        while (true) {
            ConsumerRecords<String, GenericRecord> records = consumer.poll(Duration.ofSeconds(1));
            
	    records.forEach(record -> {
                // Process the object
		System.out.printf("offset = %d, key = %s, value = %s \n", record.offset(), record.key(), record.value());            });
        }
    }
}
```

Ulangan.java

```
package com.example;

import org.apache.avro.Schema;

public class Ulangan {
    private String id;
    private double nilai;

    public Ulangan() {
    }

    public Ulangan(String id, double nilai) {
        this.id = id;
        this.nilai = nilai;
    }

    public String getId() {
        return id;
    }

    public void setId(String id) {
        this.id = id;
    }

    public double getNilai() {
        return nilai;
    }

    public void setNilai(double nilai) {
        this.nilai = nilai;
    }

    public Schema getSchema() {
        // Define the Avro schema for the Ulangan record
        String avroSchema = "{"
                + "\"type\":\"record\","
                + "\"name\":\"Ulangan\","
                + "\"fields\":["
                + "{\"name\":\"id\",\"type\":\"string\"},"
                + "{\"name\":\"nilai\",\"type\":\"double\"}"
                + "]}";
        return new Schema.Parser().parse(avroSchema);
    }
}
```

Jangan lupa tambahkan dependency untuk avro, avro serializer dan schema registry berikut:

```
<dependency>
    	    <groupId>org.apache.avro</groupId>
    	    <artifactId>avro</artifactId>
    	    <version>1.11.3</version>
	</dependency>

	<dependency>
    	    <groupId>io.confluent</groupId>
    	    <artifactId>kafka-avro-serializer</artifactId>
    	    <version>7.6.0</version>
	</dependency>

	<dependency>
	    <groupId>io.confluent</groupId>
	    <artifactId>kafka-schema-registry-client</artifactId>
	    <version>7.6.0</version>
	</dependency>
```

lakukan compile menggunakan maven ```mvn clean package``` hingga bentuk jar, dan jalankan class ```ProducerApp``` untuk produce dan ```ConsumerApp``` untuk consume message.

Berikut adalah hasil running kedua aplikasinya:

![image](https://github.com/ferdyansahalfariz/belajar-linux/assets/96871156/16bde2cf-45c3-40f2-a1a9-0b485241a075)

bagian kiri menunjukan producer berhasil produce dengan offset terakhir 27 dan dikanan menunjukan consumer berhasil menerima message nya dengan offset terakhir 27. Tidak lupa validasi juga melalui c3 berikut:

![image](https://github.com/ferdyansahalfariz/belajar-linux/assets/96871156/98038185-3418-47b0-bec7-7af7a275465c)


# Backup ke github
Sebagai cara untuk mencadangkan data untuk pencegahan jika terjadi sesuatu kepada server, maka backup perlu dilakukan. salah satu caranya yaitu dengan melalui git

1. Langkah pertama yaitu lakukan instalasi git pada server dengan perintah ```sudo yum install git```

2. setelah memastikan sudah terinstall, maka masuk ke lokasi direktori atau file yang ingin di backup, hal ini biasa terjadi untuk mencadangkan beberapa file konfigurasi ataupun file penting lainnya.

3. setelah diarahkan ke lokasi yang tepat, jalankan perintah ```git init``` untuk inisialisasi

4. lakukan perintah ```git add .``` untuk menambahkan keseluruhan file dalam direktori yang diinginkan untuk dimasukan ke repository sementara git.

5. lanjut dengan ```git commit -M "isi commit"``` isi commit dapat diisi pesan atau catatan yang dapat menandakan suatu perubahan atau update tertentu

6. pilih dimana data akan dimasukan ke branch, untuk awal saya akan menggunakan di branch main, jadi lakukan perintah ```git branch -M main```

7. kemudian disusul dengan perintah ```git remote add origin <remote-repository-URL>```. repository url diisikan dengan alamat repository yang diinginkan untuk tempat backup data disimpan.

8. terakhir lakukan perintah ```git push -u origin main``` untuk upload data yang ada di repository sementara git ke repository yang sudah ditentukan melalui perintah remote tadi. pada bagian ini, biasanya akan diminta kredensial baik username atau password github, jadi pastikan masukan akun yang dapat mengakses repository yang digunakan pada perintah ```git remote``` sebelumnya.

sebagai contoh, saya telah melakukan backup data untuk aplikasi producer dan consumer avro dengan java ke dalam repository git berikut :

```
https://github.com/ferdyansahalfariz/producer-consumer/tree/main
```

# Compatibility

Kompatibilitas skema merujuk pada aturan yang mengatur bagaimana skema baru dapat diperkenalkan dalam sistem tanpa merusak aplikasi yang ada. Confluent Schema Registry mendukung beberapa tingkat kompatibilitas:

1. **Backward Compatibility**: Skema baru dapat dibaca oleh konsumen yang mengharapkan skema lama. Dengan kata lain, skema baru hanya dapat menambahkan bidang baru yang memiliki nilai default dan tidak menghapus atau mengubah bidang yang ada.

2. **Forward Compatibility**: Skema lama dapat dibaca oleh konsumen yang mengharapkan skema baru. Ini berarti skema baru hanya dapat menghapus bidang yang ada atau menambahkan bidang baru yang tidak memiliki nilai default.

3. **Full Compatibility**: Kombinasi dari backward dan forward compatibility. Skema baru dapat dibaca oleh konsumen yang mengharapkan skema lama dan sebaliknya. Ini berarti bahwa perubahan skema harus memenuhi kedua aturan di atas.

4. **Backward Transitive Compatibility**: Sama seperti backward compatibility, tetapi diterapkan pada seluruh sejarah skema, bukan hanya skema sebelumnya.

5. **Forward Transitive Compatibility**: Sama seperti forward compatibility, tetapi diterapkan pada seluruh sejarah skema, bukan hanya skema sebelumnya.

6. **Full Transitive Compatibility**: Kombinasi dari backward transitive dan forward transitive compatibility.

7. **None**: Tidak ada aturan kompatibilitas yang diterapkan. Setiap skema baru dapat diperkenalkan tanpa memeriksa kompatibilitas dengan skema sebelumnya.

Secara default, kompatibilitas diatur ke BACkWARD dan untuk kafka stream, hanya kompatibilitas FULL, TRANSITIVE, dan BACKWARD yang diizinkan.

Sebagai praktek, kompatibilitas dari schema registry yang sudah di daftarkan dapat dirubah, pertama, saya akan menggunakan topic yang telah terdaftar bernama test dan menginput schema terlebih dahulu dengan perintah:

```
curl -X POST -H "Content-Type: application/vnd.schemaregistry.v1+json" --data '{"schema": "{\"type\":\"record\",\"name\":\"Payment\",\"namespace\":\"my.examples\",\"fields\":[{\"name\":\"id\",\"type\":\"string\"},{\"name\":\"amount\",\"type\":\"double\"}]}"}' http://master.k8s.alldataint.com:8081/subjects/test-value/versions
```

setelah itu, schema akan terdaftar dan outputnya yaitu ```{"id":5}``` yang berarti schema idnya bernilai 5. dapat dilihat juga pada c3 yang menunjukkan Compatibility mode bernilai Backward secara default.

![image](https://github.com/ferdyansahalfariz/belajar-linux/assets/96871156/6502b270-246f-4e34-8ce6-c274f72a8752)

untuk merubah mode compatibility, menggunakan perintah:

```
curl -X PUT -H "Content-Type: application/vnd.schemaregistry.v1+json" --data '{"compatibility": "FULL"}' http://master.k8s.alldataint.com:8081/config/test-value
```

dari situ maka output akan menghasilkan ```{"compatibility":"FULL"}``` yang menandakan kompatibility sudah berubah. dapat di cek pada c3:

![image](https://github.com/ferdyansahalfariz/belajar-linux/assets/96871156/6a149f75-8899-4e26-a006-a58baacd559c)

terlihat bahwa compatibility mode sudah berubah menjadi FULL.

untuk mengecek kompatibility yang dimiliki suatu schema dapat menjalankan perintah :

```
curl -X GET http://master.k8s.alldataint.com:8081/config/test-value
```

sementara untuk menghapus semua schema yang sudah terdaftar dalam subject topic dapat menggunakan perintah:

```
curl -X DELETE http://master.k8s.alldataint.com:8081/subjects/test-value
```

mengecek tipe schema apa saja yang terdaftar:

```
curl -X GET http://master.k8s.alldataint.com:8081/schemas/types
```

output:

["JSON", "PROTOBUF", "AVRO"]


# Schema Evolution
Evolusi skema adalah proses memperbarui skema untuk memenuhi kebutuhan bisnis yang berubah sambil menjaga kompatibilitas dengan data yang sudah ada. Beberapa prinsip penting dalam evolusi skema adalah:

**Menambahkan Bidang**: Menambahkan bidang baru dengan nilai default memungkinkan skema baru tetap kompatibel dengan skema lama (backward compatibility).

**Menghapus Bidang**: Menghapus bidang yang tidak digunakan dapat dilakukan dengan memastikan bahwa data lama yang dihasilkan dengan skema sebelumnya masih dapat dibaca oleh konsumen yang mengharapkan skema baru (forward compatibility).

**Mengubah Tipe Data**: Mengubah tipe data dari suatu bidang harus dilakukan dengan hati-hati. Misalnya, perubahan dari tipe int ke long mungkin kompatibel, tetapi perubahan dari string ke int mungkin tidak.

**Mengganti Nama Bidang**: Mengganti nama bidang dapat dianggap sebagai penghapusan satu bidang dan penambahan bidang baru. Ini mungkin memerlukan perubahan pada semua konsumen dan produsen data.

Untuk implementasinya sendiri, evolusi skema memiliki arti terjadinya evolusi atau perubahan pada skema baik itu menambah atau mengurangi variabel yang ada dan lain sebagainya.

### praktek

1. buat topic baru sekaligus produce message dengan mencantumkan skema. sebagai contoh saya akan membuat topic ```transactions-avro``` dengan skema yang terdiri dari atribut ```id``` dan ```amount``` berikut :

```
kafka-avro-console-producer --bootstrap-server master.k8s.alldataint.com:9092 --property schema.registry.url=http://master.k8s.alldataint.com:8081 --topic transactions-avro --property value.schema='{"type":"record","name":"Transaction","fields":[{"name":"id","type":"string"},{"name": "amount", "type": "double"}]}'
```

2. masukan data ```{ "id":"1000", "amount":550 }``` sebagai data yang di produce.

3. lakukan evolusi schema yang sudah ada dengan menambahkan atribut ```customer_id``` pada field dengan mencantumkan ```default=null``` sebagai optional karena skema menerapkan compatibility mode BACKWARD. berikut perintahnya:

```
kafka-avro-console-producer --bootstrap-server master.k8s.alldataint.com:9092 --property schema.registry.url=http://master.k8s.alldataint.com:8081 --topic transactions-avro --property value.schema='{"type": "record","name": "Transaction","fields": [{"name": "id", "type": "string"},{"name": "amount", "type": "double"},{"name": "customer_id", "type": "string", "default":null}]}'
```

4. Kemudian coba input data lagi menggunakan format data yang baru seperti ```{ "id":"1001", "amount":500, "customer_id":"1221" }```

5. Lakukan consume data menggunakan consumer dengan perintah :

```
kafka-avro-console-consumer --bootstrap-server master.k8s.alldataint.com:9092 --from-beginning --topic transactions-avro --property schema.registry.url=http://master.k8s.alldataint.com:8081
```

Hasil menunjukan terdapat perbedaan atribut data yang ada, hal ini terjadi karena schema sudah berhasil melakukan evolusi.

![image](https://github.com/ferdyansahalfariz/belajar-linux/assets/96871156/3ce1fa53-2648-4c93-b248-154d8370528b)

isi dari skema pada setiap versinya dapat dilihat dengan melakukan perintah berikut, sebagai contoh untuk versi 1:

```
curl --silent http://master.k8s.alldataint.com:8081/subjects/transactions-avro-value/versions/1/schema
```

output:

![image](https://github.com/ferdyansahalfariz/belajar-linux/assets/96871156/68e564f8-dd18-411d-9a0f-7cb0f3841d2a)

versi 2 :

```
curl --silent http://master.k8s.alldataint.com:8081/subjects/transactions-avro-value/versions/2/schema
```

output:

![image](https://github.com/ferdyansahalfariz/belajar-linux/assets/96871156/54d884b7-51b3-499f-9095-b62fd87f0ac9)

dan dapat di cek berdasarkan versi terbarunya sekaligus dipadukan dengan jq agar lebih jelas untuk membacanya sebagai berikut:

```
curl --silent -X GET http://master.k8s.alldataint.com:8081/subjects/transactions-avro-value/versions/latest | jq .
```

![image](https://github.com/ferdyansahalfariz/belajar-linux/assets/96871156/d4b4d3d7-29d3-4897-9d24-cd57ea5f42d6)

### Cek via C3

berikut adalah isi messagenya:

![image](https://github.com/ferdyansahalfariz/belajar-linux/assets/96871156/6016a0b6-0002-40fd-8653-df135fa4b92b)

pada bagian schema, terdapat 2 version yang di ada dan untuk current menggunakan version 2

![image](https://github.com/ferdyansahalfariz/belajar-linux/assets/96871156/38d82ad1-045b-46cc-91ea-958c93d724f0)

version yang ada juga dapat di compare dengan menekan compare version, dan ceklis pada turn on version diff untuk melihat secara detail dimana terjadinya perubahan pada schema.

![image](https://github.com/ferdyansahalfariz/belajar-linux/assets/96871156/aec91e39-ab0d-48ba-8a54-3da791820f21)


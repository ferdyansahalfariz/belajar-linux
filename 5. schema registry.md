# Schema Registry

![image](https://github.com/ferdyansahalfariz/belajar-linux/assets/96871156/cd9bbf94-4b0a-4d41-9314-7ff531e2ed28)

Schema Registry adalah komponen penting dalam ekosistem Apache Kafka yang memungkinkan pengguna untuk menyimpan skema data yang digunakan untuk serialisasi dan deserialisasi pesan yang diproduksi dan dikonsumsi oleh Kafka. Berikut adalah penjelasan singkat tentang konsep Schema Registry:

Definisi Skema: Sebelum mengirimkan data ke Kafka, data harus diserialisasi menjadi format yang dapat dibaca oleh Kafka. Schema digunakan untuk menentukan struktur data yang digunakan untuk serialisasi. Contoh format skema yang umum digunakan adalah Avro, JSON, atau Protobuf.

Penyimpanan Skema: Schema Registry menyimpan semua skema yang digunakan dalam ekosistem Kafka. Ini memungkinkan aplikasi pengirim dan penerima untuk secara dinamis mengambil skema yang diperlukan untuk membaca dan menulis data.

Evolutionary Compatibility: Salah satu fitur utama dari Schema Registry adalah kemampuannya untuk mengelola evolusi skema. Ini berarti Anda dapat melakukan perubahan pada skema yang ada tanpa mempengaruhi aplikasi yang sudah ada yang menggunakan skema lama.

Validasi Skema: Schema Registry juga dapat digunakan untuk memvalidasi data yang dikirimkan ke Kafka berdasarkan skema yang terdaftar. Ini membantu memastikan bahwa data yang dikirimkan sesuai dengan format yang diharapkan.

Interoperabilitas: Schema Registry mendukung beberapa format skema yang populer, seperti Avro, JSON, dan Protobuf, sehingga memungkinkan aplikasi yang menggunakan format skema yang berbeda untuk berinteraksi dengan Kafka.

Dengan demikian, Schema Registry memberikan manfaat besar dalam manajemen skema data dalam ekosistem Kafka, memastikan konsistensi dan interoperabilitas antara aplikasi yang berbeda.

## Produce & consume Avro via CLI
untuk pemebelajaran mengenai schema registry, kali ini saya akan mulai dengan membuat producer dan consumer via CLI untuk publish maupun subscribe message berformat Avro. avro sendiri merupakan salah satu tipe message yang dapat dikirim melalui kafka dan membutuhkan validasi dari schema yang ada. karenanya, diperlukan pengecekan ke schema registry terlebih dahulu sebelum melakukan publish ataupun subscribe.

### buat schemanya dan input ke kafka
skema yang digunakan sebagai berikut:
```
{
  "type": "record",
  "name": "Ulangan",
  "fields": [
    {"name": "id", "type": "string"},
    {"name": "nilai", "type": "double"}
  ]
}
```

tahap pertama yaitu input schema yang diinginkan dengan perintah :
```
curl -X POST -H "Content-Type: application/vnd.schemaregistry.v1+json" \
     --data "{\"schema\": \"{\\\"type\\\":\\\"record\\\",\\\"name\\\":\\\"Ulangan\\\",\\\"fields\\\":[{\\\"name\\\":\\\"id\\\",\\\"type\\\":\\\"string\\\"},{\\\"name\\\":\\\"nilai\\\",\\\"type\\\":\\\"double\\\"}]}\"}" \
     http://master.k8s.alldataint.com:8081/subjects/ulangan-value/versions
```
dari perintah tersebut, data berupa skema yang diinginkan diinput kedalam service schema registry dengan port 8081. Kemudian setelah berhasil maka terdapat tanda {"id":1}[root@master /]# yang menandakan schema registry sudah berhasil diinput.

![image](https://github.com/ferdyansahalfariz/belajar-linux/assets/96871156/a9082e09-7290-4710-b0e7-c21f34e17c97)

### Produce message
selanjutnya lakukan produce message dengan perintah:
```
kafka-avro-console-producer --broker-list master.k8s.alldataint.com:9092 --topic ulangan --property schema.registry.url=http://master.k8s.alldataint.com:8081 --property value.schema.id=1
```
atau dapat langsung memasukan message dan schema yang diinginkan dengan perintah:
```
echo "{\"id\": \"2\", \"nilai\": 90.0}" | kafka-avro-console-producer --broker-list master.k8s.alldataint.com:9092 --topic ulangan --property schema.registry.url=http://master.k8s.alldataint.com:8081 --property value.schema='{"type":"record","name":"Ulangan","fields":[{"name":"id","type":"string"},{"name":"nilai","type":"double"}]}'
```
property schema registry url dan value schema id dibutuhkan untuk menerapkan schema yang akan dijadikan blueprint terhadap message yang akan dikirim. lakukan publish message yang sudah sesuai dengan skemam jika tidak maka otomatis akan gagal.

jika message tak sesuai format skema, maka akan ditolak dan muncul error berikut:

![image](https://github.com/ferdyansahalfariz/belajar-linux/assets/96871156/61f1e2bc-a17b-4e13-a27d-f543366c54ea)

### Consume message
untuk consume data avro lakukan perintah :
```
kafka-avro-console-consumer --bootstrap-server master.k8s.alldataint.com:9092 --topic ulangan --from-beginning --property schema.registry.url=http://master.k8s.alldataint.com:8081 --property value.schema.id=1
```

![image](https://github.com/ferdyansahalfariz/belajar-linux/assets/96871156/1c612ad8-a58f-4839-86f5-219d530241a6)

## Membuat program produce/consume avro dengan java
Terdapat sedikit perbedaan yang membedakan produce/consume message menggunakan avro yang mana harus membuat class yang merepresentasikan skema yang diinginkan, selain itu terdapat penerapan kafkaAvroSerializer untuk menerjemahkan message dari atau ke format avro.

Berikut adalah isi programnya:

ProducerApp.java

```
package com.example;

import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.Producer;
import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.apache.kafka.clients.producer.RecordMetadata;
import org.apache.avro.generic.GenericData;
import org.apache.avro.generic.GenericRecord;
import io.confluent.kafka.serializers.KafkaAvroSerializer;

import java.util.Properties;
import java.io.FileInputStream;
import java.io.IOException;
import java.util.concurrent.ExecutionException;

public class ProducerApp {

    public static void main(String[] args) {
        String bootstrapServers = "master.k8s.alldataint.com:9092";

        String schemaRegistryUrl = "http://master.k8s.alldataint.com:8081";

        Properties properties = new Properties();
        properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);
        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, KafkaAvroSerializer.class.getName());
        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, KafkaAvroSerializer.class.getName());
        properties.put("schema.registry.url", schemaRegistryUrl);

        try (KafkaProducer<String, GenericRecord> producer = new KafkaProducer<>(properties)) {
            double nilaiAwal = 80.00;
            for (int i = 1; i <= 5; i++) {
                    Ulangan ulangan = new Ulangan(String.valueOf(i), nilaiAwal);

                    GenericRecord genericRecord = new GenericData.Record(ulangan.getSchema());
                    genericRecord.put("id", ulangan.getId());
                    genericRecord.put("nilai", ulangan.getNilai());

                    ProducerRecord<String, GenericRecord> record = new ProducerRecord<>("ulangan", Integer.toString(i), genericRecord);
                    producer.send(record, (RecordMetadata metadata, Exception exception) -> {
                if (exception == null) {
                        System.out.printf("Avro record terkirim ke partisi %d dengan offset %d%n : ", metadata.partition(), metadata.offset());
                } else {
                    exception.printStackTrace();
                }
            });
            nilaiAwal += 5;
            }
        } catch (Exception e) {
            e.printStackTrace();
        }

    }
}
```

ConsumerApp.java

```
package com.example;

import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.clients.consumer.KafkaConsumer;
import org.apache.avro.generic.GenericRecord;
import io.confluent.kafka.serializers.KafkaAvroDeserializer;

import java.time.Duration;
import java.util.Collections;
import java.util.Properties;
import java.util.List;
import java.io.FileInputStream;
import java.io.IOException;

public class ConsumerApp {

    public static void main(String[] args) {
        String bootstrapServers = "master.k8s.alldataint.com:9092";

        String schemaRegistryUrl = "http://master.k8s.alldataint.com:8081";

        Properties properties = new Properties();
        properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);
        properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, KafkaAvroDeserializer.class.getName());
        properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, KafkaAvroDeserializer.class.getName());
        properties.put("schema.registry.url", schemaRegistryUrl);
        properties.put(ConsumerConfig.GROUP_ID_CONFIG, "ulangan");
        properties.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");

        KafkaConsumer<String, GenericRecord> consumer = new KafkaConsumer<>(properties);
        consumer.subscribe(List.of("ulangan"));

        while (true) {
            ConsumerRecords<String, GenericRecord> records = consumer.poll(Duration.ofSeconds(1));
            
	    records.forEach(record -> {
                GenericRecord avroRecord = record.value();

    	String key = record.key();
    	long offset = record.offset();

                Ulangan ulangan = new Ulangan();
                ulangan.setId(avroRecord.get("id").toString());
                ulangan.setNilai((Double) avroRecord.get("nilai"));

		System.out.println("Key: " + key + ", Offset: " + offset + ", Ulangan: " + ulangan.toString());
            });
        }
    }
}
```

Ulangan.java

```
package com.example;

import org.apache.avro.Schema;

public class Ulangan {
    private String id;
    private double nilai;

    public Ulangan() {
    }

    public Ulangan(String id, double nilai) {
        this.id = id;
        this.nilai = nilai;
    }

    public String getId() {
        return id;
    }

    public void setId(String id) {
        this.id = id;
    }

    public double getNilai() {
        return nilai;
    }

    public void setNilai(double nilai) {
        this.nilai = nilai;
    }

    public Schema getSchema() {
        // Define the Avro schema for the Ulangan record
        String avroSchema = "{"
                + "\"type\":\"record\","
                + "\"name\":\"Ulangan\","
                + "\"fields\":["
                + "{\"name\":\"id\",\"type\":\"string\"},"
                + "{\"name\":\"nilai\",\"type\":\"double\"}"
                + "]}";
        return new Schema.Parser().parse(avroSchema);
    }
}
```

Jangan lupa tambahkan dependency untuk avro, avro serializer dan schema registry berikut:

```
<dependency>
    	    <groupId>org.apache.avro</groupId>
    	    <artifactId>avro</artifactId>
    	    <version>1.11.3</version>
	</dependency>

	<dependency>
    	    <groupId>io.confluent</groupId>
    	    <artifactId>kafka-avro-serializer</artifactId>
    	    <version>7.6.0</version>
	</dependency>

	<dependency>
	    <groupId>io.confluent</groupId>
	    <artifactId>kafka-schema-registry-client</artifactId>
	    <version>7.6.0</version>
	</dependency>
```

lakukan compile menggunakan maven ```mvn clean package``` hingga bentuk jar, dan jalankan class ```ProducerApp``` untuk produce dan ```ConsumerApp``` untuk consume message.

Berikut adalah hasil running kedua aplikasinya:

![image](https://github.com/ferdyansahalfariz/belajar-linux/assets/96871156/16bde2cf-45c3-40f2-a1a9-0b485241a075)

bagian kiri menunjukan producer berhasil produce dengan offset terakhir 27 dan dikanan menunjukan consumer berhasil menerima message nya dengan offset terakhir 27. Tidak lupa validasi juga melalui c3 berikut:

![image](https://github.com/ferdyansahalfariz/belajar-linux/assets/96871156/98038185-3418-47b0-bec7-7af7a275465c)


# Backup ke github
Sebagai cara untuk mencadangkan data untuk pencegahan jika terjadi sesuatu kepada server, maka backup perlu dilakukan. salah satu caranya yaitu dengan melalui git

1. Langkah pertama yaitu lakukan instalasi git pada server dengan perintah ```sudo yum install git```

2. setelah memastikan sudah terinstall, maka masuk ke lokasi direktori atau file yang ingin di backup, hal ini biasa terjadi untuk mencadangkan beberapa file konfigurasi ataupun file penting lainnya.

3. setelah diarahkan ke lokasi yang tepat, jalankan perintah ```git init``` untuk inisialisasi

4. lakukan perintah ```git add .``` untuk menambahkan keseluruhan file dalam direktori yang diinginkan untuk dimasukan ke repository sementara git.

5. lanjut dengan ```git commit -M "isi commit"``` isi commit dapat diisi pesan atau catatan yang dapat menandakan suatu perubahan atau update tertentu

6. pilih dimana data akan dimasukan ke branch, untuk awal saya akan menggunakan di branch main, jadi lakukan perintah ```git branch -M main```

7. kemudian disusul dengan perintah ```git remote add origin <remote-repository-URL>```. repository url diisikan dengan alamat repository yang diinginkan untuk tempat backup data disimpan.

8. terakhir lakukan perintah ```git push -u origin main``` untuk upload data yang ada di repository sementara git ke repository yang sudah ditentukan melalui perintah remote tadi. pada bagian ini, biasanya akan diminta kredensial baik username atau password github, jadi pastikan masukan akun yang dapat mengakses repository yang digunakan pada perintah ```git remote``` sebelumnya.

sebagai contoh, saya telah melakukan backup data untuk aplikasi producer dan consumer avro dengan java ke dalam repository git berikut :

```
https://github.com/ferdyansahalfariz/producer-consumer/tree/main
```

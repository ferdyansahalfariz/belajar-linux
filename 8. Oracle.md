# Oracle

Pada kesempatan kali ini, saya akan mempelajari bagaimana cara instalasi Oracle, dan membuat CDC Source dari database Oracle ke kafka topic dengan menggunakan kafka connector.

## Instalasi Oracle

Pertama, saya menggunakan VM untuk mendownload Oraclenya dan diakses melalui Remote dekstop connection (RDC). masuk ke rdc dengan kredensial yang tepat lalu lakukan download terlebih dahulu database oracle berbentuk ```zip``` di website resminya, pada kesempatan ini, saya menggunakan versi 19c.

Setelah itu, extract ditempat yang diinginkan lalu jalankan ```setup.exe```. Lakukan prosesnya sampai selesai. 

Buat listener baru dengan perintah netca pada cmd lalu ikuti langkahnya sampai selesai.

Buat juga database baru dengan perintah dbca di cmd lalu kustomisasi database sesuai dengan kebutuhan. jangan lupa untuk mencatat kredensialnya juga.

Setelah database dibuat, coba sambungkan ke dbeaver dengan alamat dan kredensial yang sesuai dengan database baik itu host, nama db, username, password dan jangan lupa masuk sebagai sysdba yang memiliki ibarat sebagai root atau superadmin.

Lakukan pembuatan user baru selain dari paa menggunakan ```sys``` yang mana juga berlaku sebagai skema di database oracle, dari skema atau user tersebut lakukan pembuatan tabel sesuai kebutuhan dan isikan dengan data yang diinginkan, pastikan berhasil masuk dan tidak terjadi error.

Tutorial selengkapnya: https://www.youtube.com/watch?v=W3r8shGn2mY

## Database Pre requisites
referensi: [db pre req](https://docs.confluent.io/kafka-connectors/oracle-cdc/current/prereqs-validation.html#validate-start-up-configuration-and-prerequisite-completion)

Pada database Oracle, terdapat beberapa konfigurasi tambahan agar database dapat digunakan untuk melakukan cdc source pada kafka, lakukan persiapan pada database seperti contoh pada referensi diatas yang terdiri dari konfigurasi user privileges, menyalakan ARCHIVELOG mode, serta enable suplemental logging. Hal ini berguna agar kafka dapat terhubung ke oracle dengan memantau log yang ada dari database oracle termasuk pada tiap perubahan yang terjadi.

## Sambung ke CDC Source Connector
Download terlebih dahulu connectornya di ```https://www.confluent.io/hub/``` lalu lakukan extract ke direktori yang ada di ```connect-distributed.properties``` seperti pada materi 7.connect. 

Setelah memastikan sudah ada, buka C3 lalu import konfigurasi untuk membuat connectornya dengan konfigurasi berikut:

```
{
  "name": "cdc-source-oracle-mahasiswa",
  "config": {
    "value.converter.schema.registry.url": "http://master.k8s.alldataint.com:8081",
    "key.converter.schema.registry.url": "http://master.k8s.alldataint.com:8081",
    "name": "cdc-source-oracle-mahasiswa",
    "connector.class": "io.confluent.connect.oracle.cdc.OracleCdcSourceConnector",
    "tasks.max": "1",
    "key.converter": "io.confluent.connect.avro.AvroConverter",
    "value.converter": "io.confluent.connect.avro.AvroConverter",
    "transforms": "setSchema",
    "errors.retry.timeout": "60000",
    "errors.retry.delay.max.ms": "5000",
    "errors.tolerance": "all",
    "transforms.setSchema.type": "org.apache.kafka.connect.transforms.SetSchemaMetadata$Value",
    "transforms.setSchema.schema.name": "skemaMhs",
    "oracle.server": "10.100.13.32",
    "oracle.port": "1521",
    "oracle.sid": "ora19",
    "oracle.username": "sys as sysdba",
    "oracle.password": "Alldata123#",
    "redo.log.consumer.bootstrap.servers": "master.k8s.alldataint.com:9092",
    "table.inclusion.regex": "ora19[.]FERDY[.](MAHASISWA)"
  }
}
```

Launch connectornya dan pastikan untuk service kafka dan schema registry sudah running. Jika tidak ada satupun error, konfigurasi tersebut akan secara otomatis membuat topic baru hasil dari souce pada database oracle tersebut. Pada kasus ini saya menggunakan database dengan nama ```ora19```, dan skema atau user ```FERDY``` untuk membuat tabel dengan nama ```MAHASISWA```.

![image](https://github.com/ferdyansahalfariz/belajar-linux/assets/96871156/7394364b-87da-4fcf-b203-b54b47d2d698)

Jika topic sudah ada, dapat dicek juga pada skema yang mana sudah teregister juga.

![image](https://github.com/ferdyansahalfariz/belajar-linux/assets/96871156/67cf2db1-9d74-40fb-87e3-72a44856e201)

# 26 Juni 2024
Sebagai kelanjutan pembelajaran cdc source oracle, saya yang sebelumnya hanya melakukan cdc pada 1 tabel saja dalam database ```ora19``` dan skema ```FERDY```, saat ini akan mencoba untuk melakukan cdc pada multiple table. 

Dalam hal ini saya akan melakukan cdc pada 4 tabel yaitu tabel ```cdc_mahasiswa```, ```cdc_dosen```, ```cdc_matakuliah``` dan ```cdc_kelas```. untuk proses konfigurasinya sendiri, terdapat perbedaan dari value di ```table.inclusion.regex``` nya yang mana kita harus menyebutkan tiap tabel yang ingin digunakan sebagai soure untuk cdc ke kafka topic.

Berikut adalah perbedaan konfigurasi saya dari konfigurasi sebelumnya:

```
{
  "table.inclusion.regex": "ora19[.]FERDY[.](CDC_MAHASISWA|CDC_DOSEN|CDC_MATAKULIAH|CDC_KELAS)"
}
```

Sebelum melakukan import connectornya, pastikan bahwa tabel di database oracle sudah ada, lalu mulai import config connector baru, make sure tidak terjadi error dan jika terjadi, lakukan pemeriksaan log pada kafka connectnya dengan ```sudo journalctl -f -u confluent-kafka-connect``` sebelum launch connector barunya.

Sebagai hasil dari connector tersebut, akan muncul topic baru sejumlah total tabel dan sesuai dengan namanya seperti berikut:

![image](https://github.com/ferdyansahalfariz/belajar-linux/assets/96871156/f40f20e2-4e0a-4b46-bd74-ac8f91a96def)

Cek satu persatu topic apakah isi datanya juga sudah lengkap dan sesuai dengan isi tabel di database oracle. Pastikan juga skema yang ter regist sudah sesuai dengan data yang ada di tabel database oracle.

Berikut adalah salah satu isi event di kafka topic:

```
{
  "NID": "D004",
  "NAMA": {
    "string": "Prof. joko"
  },
  "ALAMAT": {
    "string": "Jl. pahlawan no 2"
  },
  "RATING": {
    "double": 4.9
  },
  "table": {
    "string": "ora19.FERDY.CDC_DOSEN"
  },
  "scn": {
    "string": "2417734"
  },
  "op_type": {
    "string": "D"
  },
  "op_ts": {
    "string": "1719396823000"
  },
  "current_ts": {
    "string": "1719371692137"
  },
  "row_id": {
    "string": "AAAR4uAAHAAAAFuAAE"
  },
  "username": {
    "string": "SYS"
  }
}
```

Untuk isi dari event di kafka topic sendiri, terdapat beberapa detail tambahan yang tercatat selain dari pada isi field yang ada di tabel, seperti "table", "op_type" "scn", "op_type", "op_ts", "current_ts", "row_id", dan "username".

Penjelasan:
* tabel: menunjukan di mana posisi tabel terjadinya event
* op_type: menunjukkan tipe operasi nya, jika data row sudah ada sebelum connector terpasang maka akan berisi "R" atau read, lalu jika connector sudah terpasang akan muncul "I" untuk insert,"U" untuk update dan "D" untuk delete.
* scn: Nomor sistem change number (SCN) yang menandakan versi dari perubahan data.
* op_ts: Timestamp ketika operasi terjadi.
* current_ts: Timestamp saat event ini diproses oleh Kafka Connect.
* row_id: ID unik untuk baris yang dihapus.
* username: Pengguna yang melakukan operasi (dalam kasus ini, "SYS").

Berikut adalah contoh untuk event yang masuk ke kafka topic setiap terjadinya perubahan pada isi tabel cdc_dosen di database:

![image](https://github.com/ferdyansahalfariz/belajar-linux/assets/96871156/e3a5ea8e-ddb5-4e68-8a89-0b5437ac0352)

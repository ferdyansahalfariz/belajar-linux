# KSQLDB

Sumber : 

* [ksql](https://ksqldb.io/)

* [youtube](https://www.youtube.com/watch?v=7mGBxG2NhVQ&t=3685s)

KSQL adalah platform streaming SQL yang dibangun di atas Apache Kafka. Ini memungkinkan pengguna untuk menulis kueri SQL untuk memproses aliran data secara real-time yang disimpan di dalam topik Kafka. Dengan menggunakan kueri SQL yang familiar, pengguna dapat melakukan berbagai operasi pada aliran data seperti filter, join, agregasi, transformasi, dan lainnya.

Berikut adalah beberapa fitur utama dari KSQL:

**Kueri SQL untuk Streaming Data**: Pengguna dapat menulis kueri SQL untuk memproses aliran data yang masuk dan menghasilkan hasil secara real-time.

**Sink dan Source**: KSQL mendukung konfigurasi sink dan source untuk mentransfer data dari dan ke Kafka. Ini memungkinkan integrasi dengan sistem eksternal untuk memasukkan atau mengambil data dari Kafka.

**Operasi Streaming**: KSQL mendukung operasi streaming seperti SELECT, JOIN, FILTER, AGGREGATE, dan lainnya, yang memungkinkan pengguna untuk melakukan analisis data yang kompleks dalam aliran data secara real-time.

**Integrasi dengan Kafka Connect**: KSQL dapat berintegrasi dengan Kafka Connect untuk mengakses sumber data eksternal dan menyimpan hasil kueri ke penyimpanan eksternal.

**Antarmuka CLI dan REST**: KSQL menyediakan antarmuka baris perintah (CLI) dan REST API yang memungkinkan pengguna untuk berinteraksi dengan KSQL dan menjalankan kueri SQL.

Dengan KSQL, pengguna dapat dengan mudah menerapkan logika bisnis kompleks pada aliran data secara real-time tanpa perlu menulis kode yang kompleks atau menggunakan alat yang rumit.

Adapun beberapa penjelasan penting mengenai isi dari ksql antaranya yaitu :

**Stream**: Representasi berkelanjutan dari data yang masuk. Data dalam stream bergerak dari satu titik ke titik lainnya dan dapat terus bertambah seiring waktu. Misalnya, stream data sensor suhu yang terus mengirimkan data suhu.

**Table**: Representasi data yang diperbarui secara teratur yang dapat diquery pada waktu tertentu. Data dalam tabel adalah snapshot terbaru dari stream yang mendasarinya pada saat query dieksekusi. Misalnya, tabel informasi pengguna yang diperbarui setiap kali ada perubahan informasi pengguna.

**Transient Query**: Query yang memberikan hasil hanya untuk eksekusi query tersebut dan tidak menyimpan hasilnya. Transient query cocok digunakan untuk keperluan analisis cepat dan sementara.

**Persistent Query**: Query yang berjalan secara terus menerus dan menyimpan hasilnya. Persistent query cocok untuk keperluan pemrosesan data berkelanjutan dan mempertahankan hasil query untuk digunakan di masa mendatang.

## Praktek
Sebelum memulainya, pastikan terlebih dahulu configurasi dari ksqldb sudah sesuai dan tidak ada masalah, saat membuat dan menjalankan query di ksql, secara otomatis topic maupun schema akan dibuat, jadi pastikan url untuk schema registry sudah disesuaikan agar secara otomatis dapat register dan check schema yang ada.

Untuk memulainya, cukup ketik ksql pada server sampai muncul berikut:

![image](https://github.com/ferdyansahalfariz/belajar-linux/assets/96871156/a866ad88-d0f8-46ba-88d7-d93fcf49b488)

Pada kesempatan kali ini saya akan mencoba untuk membuat sebuah stream, tabel, persistent query, transient query, push query maupun pull query.

1. Sebagai studi kasus, saya akan membuat sebuah stream yang berisi skema berikut:

```
{
  "type": "record",
  "name": "match",
  "fields": [
    {"name": "matchId", "type": "string"},
    {"name": "team", "type": "string"},
    {"name": "score", "type": "int"}
  ]
}
```

dari skema tersebut, gunakan perintah ini untuk membuat stream:

```
CREATE STREAM match (matchId VARCHAR, team VARCHAR, score INT) WITH (kafka_topic='matches', value_format='AVRO', partitions=1);
```

ksql otomatis akan membuat topic matches dengan skema berformat avro. 

2. Untuk menampung total score yang dihasilkan setiap team yang ada, saya akan membuat tabel dengan perintah berikut:

```
CREATE TABLE currentScore AS
  SELECT team,
         SUM(score) AS score
  FROM match
  GROUP BY team
  EMIT CHANGES;
```

tabel ```currentScore``` akan menjumlahkan score dari setiap team pada tiap data yang diinput melalui stream ```match```. Perintah diatas merupakan salah satu contoh dari **persistent query**, hal ini berarti bahwa query tersebut akan bertahan seterusnya bahkan jika server dimatikan, jadi query tersebut bekerja dengan mencopy data dari stream ```match``` secara terus menerus untuk dimasukan ke tabel ```currentScore```.

3. Selanjutnya saya akan memberikan salah satu contoh dari push query. Menurut [KsqlDb](https://docs.ksqldb.io/en/latest/concepts/queries/) sendiri, push query adalah jenis query yang memungkinkan klien untuk berlangganan dan menerima hasil query secara real-time saat terjadi perubahan. Contoh penggunaan push query adalah dalam mengambil lokasi geografis pengguna yang berubah secara dinamis. Dengan menggunakan kueri ini, perubahan lokasi pengguna akan "didorong" kepada klien melalui koneksi yang berlangsung lama segera setelah perubahan terjadi. Push query dapat digunakan untuk mengambil data dari stream atau tabel berdasarkan kunci tertentu, serta mendukung operasi SQL lengkap seperti filter, select, group by, partition by, dan join. Push query sangat berguna untuk membangun aplikasi real-time dan aliran kontrol asinkron lainnya. Berikut adalah salah satu push query :

```
SELECT * FROM match
  WHERE score >= 3 EMIT CHANGES;
```

Push query tersebut merupakan query yang akan selalu menampilkan data dari stream match dengan kondisi jika terdapat team yang berhasil mencetak lebih besar atau sama dengan 3 skor, query tersebut akan selalu berjalan, namun dapat berhenti jika dihentikan dengan sengaja.

4. Tanpa menghentikan push query diatas, buka CLI session yang lain selanjutnya lakukan pengisian populasi atau input message ke dalam stream match dengan perintah:

```
INSERT INTO match (matchId, team, score) VALUES ('match1', 'barca', 3);
INSERT INTO match (matchId, team, score) VALUES ('match1', 'madrid', 1);
INSERT INTO match (matchId, team, score) VALUES ('match2', 'valencia', 3);
INSERT INTO match (matchId, team, score) VALUES ('match2', 'barca', 2);
INSERT INTO match (matchId, team, score) VALUES ('match3', 'madrid', 5);
INSERT INTO match (matchId, team, score) VALUES ('match3', 'valencia', 0);
```

Lihat kembali query push pada langkah nomer 3, seharusnya tabel sudah terisi dengan team yang berhasil mencetak skor 3 atau diatasnya seperti pada sisi kiri gambar:

![image](https://github.com/ferdyansahalfariz/belajar-linux/assets/96871156/0bfc4b36-b7b8-4116-bd8c-f82048331144)

6. sebagai perbandingan, pada gambar diatas merupakan query bertipe pull yang merupakan jenis query yang memungkinkan klien untuk mengambil hasil berdasarkan kondisi saat ini, mirip dengan query pada RDBMS tradisional. Misalnya, pull query untuk lokasi geografis akan meminta koordinat terkini dari pengguna tertentu dan segera mengembalikan hasilnya dengan koneksi yang ditutup.Jadi saat pull dipanggil, maka data yang diambil adalah data saat itu saja, untuk mendapatkan data terupdate, pull query harus dipanggil ulang. berikut adalah query pull:

```
SELECT * FROM match
  WHERE score >= 3 AND team = 'barca';
```

7. Sebagai tambahan, lakukan insert data kembali untuk menunjukan perbedaan kedua query tersebut, saya menambah data berikut:

```
INSERT INTO match (matchId, team, score) VALUES ('match4', 'barca', 5);
INSERT INTO match (matchId, team, score) VALUES ('match4', 'valencia', 0);
```

dari hal tersebut dapat dilihat di sisi kiri bahwa push query secara otomatis bertambah isinya sedangkan untuk sisi kanan, pull query harus dilakukan pemanggilan lagi baru kemudian datanya terupdate.

![image](https://github.com/ferdyansahalfariz/belajar-linux/assets/96871156/eabf8d9f-c94b-4d32-b34c-3edc374cdcd0)

Sebagai catatan, untuk pull maupun push query merupakan sebuah Transient Query yang hanya digunakan saat itu saja tanpa tersimpan di dalam server, untuk memperjelas Transient Query, saya akan membuat 1 query lagi yang berfungsi untuk menampilkan isi dari tabel currentScore yang menunjukan keseluruhan score yang diperoleh masing-masing team berikut:

```
SELECT * FROM currentScore;
```

![image](https://github.com/ferdyansahalfariz/belajar-linux/assets/96871156/92fc8fd1-5208-4513-ad23-1bf4e89df527)

## Join

[Join](https://docs.ksqldb.io/en/latest/developer-guide/joins/)

Join adalah kondisi dimana dilakukannya penggabungan antara stream dengan stream, tabel dengan tabel ataupun stream dengan tabel. terdapat beberapa aturan pada join di ksql berikut:

![image](https://github.com/ferdyansahalfariz/belajar-linux/assets/96871156/99b82522-cfff-49ca-836f-f3d198534a72)

pada prakteknya, saya mencoba untuk membuat stream baru dengan judul matchDetail yang mencatat detail dari tiap match dengan atribut ```matchDetailId```,```matchId```,```stadium```, dan ```timestamp```. berikut querynya:

```
CREATE STREAM matchDetail (
    matchDetailId STRING,
    matchId STRING,
    stadium STRING,
    timestamp STRING
) WITH (
    KAFKA_TOPIC='matchDetail',
    PARTITIONS=1,
    VALUE_FORMAT='avro',
    TIMESTAMP='timestamp',
    TIMESTAMP_FORMAT='yyyy-MM-dd''T''HH:mm:ssXXX');
```

setelah itu saya akan melakukan join terhadap 2 stream antara matchDetail dengan match yang sebelumnya telah dibuat melalui stream dengan query:

```
CREATE STREAM match_detail_stream AS
SELECT
    m.matchId,
    m.team,
    m.score,
    md.stadium,
    md.timestamp
FROM
    match m
JOIN
    matchDetail md WITHIN 100 DAYS
ON
    m.matchId = md.matchId;
```

Untuk melihat hasilnya, saya menambahkan populasi atau mengirim message dengan isi :

```
INSERT INTO match (matchId, team, score) VALUES ('match5', 'madrid', 0);
INSERT INTO match (matchId, team, score) VALUES ('match5', 'valencia', 5);
INSERT INTO matchDetail (matchDetailId, matchId, stadium, timestamp) VALUES ('matchDetail5','match5', 'harupat', '2024-06-01T16:00:00+07:00');
```

Terakhir jalankan query : ```SELECT * FROM match_detail_stream;```

Maka akan muncul tabel berikut:
![image](https://github.com/ferdyansahalfariz/belajar-linux/assets/96871156/d9708091-d2ac-46de-b774-6e5b7007bc4c)

## Flow

Jika membuka C3 pada bagian KsqlDB, maka akan terlihat bagaimana detail dari stream dan tabel yang ada lengkap dengan field yang ada di dalamnya serta persistent query yang ada. untuk flow dari studi kasus saya, berikut adalah yang ditampilkan:

![image](https://github.com/ferdyansahalfariz/belajar-linux/assets/96871156/e63fdb75-47f9-4f7e-8442-fe5ed4e93a8a)

tulisan ```create-table``` dan ```create-stream``` menunjukkan fungsi dari persistent query yang dibuat. sebagai contoh, pada proses ```create-stream``` yang menghasilkan ```match_detail_stream```, hal tersebut merupakan salah satu persistent query join yang bersumber dari dua stream yang ada yaitu ```match``` dan ```matchDetail```.


# 14 Juni 2024
# UDF

Pada kesempatan kali ini, saya akan breakdown tentang User Defined Function (UDF).User Defined Function (UDF) di ksqlDB adalah sebuah fungsi yang ditulis oleh user untuk melakukan operasi khusus yang tidak tersedia dalam fungsi bawaan ksqlDB. UDF memungkinkan pengguna untuk memperluas kemampuan ksqlDB dengan menambahkan logika khusus yang ditulis dalam Java.

Sebagai salah satu dari [custom function reference](https://docs.ksqldb.io/en/0.7.1-ksqldb/concepts/functions/), UDF merupakan stateless scalar function. Fungsi skalar adalah fungsi yang menerima satu baris input dan mengembalikan satu nilai output. Tidak ada status yang dipertahankan di antara panggilan fungsi. Ketika Anda mengimplementasikan fungsi skalar kustom, itu disebut sebagai Fungsi yang Didefinisikan Pengguna (UDF). Sebenarnya sudah terdapat fungsi bawaan yang ada di ksql yang bertipe scalar yang dapat di cek di [scalar](https://docs.ksqldb.io/en/0.7.1-ksqldb/developer-guide/ksqldb-reference/scalar-functions/#urls). Namun, dalam implementasinya, seringkali user masih butuh mempersonalisasi sebuah fungsi agar dapat memenuhi kebutuhan bisnis, dalam hal ini fungsi tidak dapat ditemukan di fungsi bawaan yang mengharuskannya untuk custom sendiri. 

Selanjutnya saya akan mencoba mendemonstrasikan langkah yang dilakukan untuk membuat custom function /UDF menggunakan java.

## Praktek

Referensi :
* [ksqldb.io](https://docs.ksqldb.io/en/latest/how-to-guides/create-a-user-defined-function/)
* [developer confluent io](https://developer.confluent.io/tutorials/udf/ksql.html)

Pada studi kasus kali ini, saya akan membuat function sederhana yang bernama ```reverse```, seperti namanya, fungsi ini berguna untuk melakukan putak balik dari value yang ada baik itu berupa string, integer, long, ataupun juga double. berikut adalah langkah-langkahnya:

1. buat project maven baru dengan menambahkan dependency berikut:

```
<!-- https://mvnrepository.com/artifact/io.confluent.ksql/ksqldb-udf -->
	<dependency>
    	    <groupId>io.confluent.ksql</groupId>
    	    <artifactId>ksqldb-udf</artifactId>
    	    <version>7.6.0</version>
	</dependency>
```

2. Buat file java baru di direktori ```src/main/java/com.example``` dengan nama ```ReverseUdf.java```.

```
package com.example;

import io.confluent.ksql.function.udf.Udf;
import io.confluent.ksql.function.udf.UdfDescription;
import io.confluent.ksql.function.udf.UdfParameter;


	@UdfDescription(
	name = "reverse",
	description = "Example UDF that reverses an object",
	version = "0.1.0",
	author = "ferdy"
	)
	public class ReverseUdf {
	@Udf(description = "Reverse a string")
		public String reverseString(@UdfParameter(value = "source", description = "the value to reverse")
		final String source) {
		return new StringBuilder(source).reverse().toString();
	}


	@Udf(description = "Reverse an integer")
	public String reverseInt(
	@UdfParameter(value = "source", description = "the value to reverse")
	final Integer source
	) {
	return new StringBuilder(source.toString()).reverse().toString();
	}


	@Udf(description = "Reverse a long")
	public String reverseLong(
	@UdfParameter(value = "source", description = "the value to reverse")
	final Long source
	) {
	return new StringBuilder(source.toString()).reverse().toString();
	}


	@Udf(description = "Reverse a double")
	public String reverseDouble(
	@UdfParameter(value = "source", description = "the value to reverse")
	final Double source
	) {
	return new StringBuilder(source.toString()).reverse().toString();
    }
}
```

Pada proses pembuatan custom function sendiri, terdapat penerapan dari anotasi ```@Udf```, ```@UdfDescription```, dan ```@UdfParameter```. Anotasi ```@UdfDescription``` menandai kelas sebagai UDF skalar. Parameter name memberikan nama pada fungsi sehingga dapat merujuknya dalam SQL. Anotasi ```@Udf``` menandai method sebagai body kode yang akan dipanggil ketika fungsi dipanggil. Dan anotasi ```@UdfParameter``` menandai setiap parameter dari metode. Ini dapat digunakan untuk memberikan beberapa metadata opsional tentang setiap parameter, tetapi ini terutama berguna agar ksqlDB dapat menyimpulkan informasi saat runtime.

referensi: [konsep function](https://docs.ksqldb.io/en/0.7.1-ksqldb/concepts/functions/)

3. Jika sudah, simpan dan jalankan project melalui maven dengan perintah ```mvn clean package``` sehingga membentuk file JAR.

4. Tambahkan konfigurasi pada ```ksql-server.properties``` untuk mengatur extension file UDF yang berupa jar agar dapat diambil dan diterapkan pada ksql, pastikan juga direktori tersebut dapat diakses oleh ksql. Berikut adalah penambahan configurasinya:

```
#------ Extension config -------
ksql.extension.dir=/udf/target
```

disini saya langsung merefer ke folder tempat jar dari reverseUdf yang sudah di ekstrak ke jar.

5. lakukan restart pada service ksqldb dan pastikan dapat berjalan tanpa adanya error.

```
systemctl restart confluent-ksqldb
systemctl status confluent-ksqldb
```

6. Start service ksql lalu coba terlebih dahulu dengan query ```SHOW FUNCTION;``` hal ini berguna untuk melihat keseluruhan function yang dapat digunakan di ksql termasuk function custom yang sudah dibuat. berikut adalah hasilnya:

![image](https://github.com/ferdyansahalfariz/belajar-linux/assets/96871156/3bd0253e-7069-4556-b1aa-8f7f81ac93bf)

7. Coba panggil query ```DESCRIBE FUNCTION reverse;``` jika sudah keluar output dibawah lengkap dengan deskripsinya, maka function sudah berjalan dan sudah dapat digunakan.

![image](https://github.com/ferdyansahalfariz/belajar-linux/assets/96871156/c232c38a-1e6d-4b85-8cca-d8d76b57ec53)

8. sebagai kelanjutannya, saya membuat stream dan topic baru dengan perintah:

```
CREATE STREAM reverse (contohString VARCHAR, contohInt INT, contohLong LONG, contohDouble DOUBLE) WITH (kafka_topic='reverse', value_format='AVRO', partitions=1);
```

9. running query push dengan ```SELECT * FROM reverse EMIT CHANGES;``` untuk mengecek secara update isi dari stream reverse sebelum dilakukannya pengisian populasi.

10. Lakukan pengisian populasinya, saya akan mengisinya 2 kali dengan perintah :

```
INSERT INTO reverse (contohString, contohInt, contohLong, contohDouble) VALUES ('novrizalferdy', 12345, 1234567890123, 123.456);
INSERT INTO reverse (contohString, contohInt, contohLong, contohDouble) VALUES ('ferdy', 6789, 67891234, 987.654);
```

dari pengisian tersebut, seharusnya query push pada langkah ke 9 sudah menampilkan isi berikut:

![image](https://github.com/ferdyansahalfariz/belajar-linux/assets/96871156/0e13baa6-60fd-4c29-b385-96b2b205d5a4)

11. Terakhir, coba langsung terapkan custom function UDF yang telah dibuat yaitu reverse kedalam query select dari stream reverse untuk melihat apakah function sudah berjalan. lakukan query berikut:

```
SELECT contohString, REVERSE(contohString) AS reversedString, contohInt, REVERSE(contohInt) AS reversedInt,contohLong, REVERSE(contohLong) AS reversedLong,contohDouble, REVERSE(contohDouble) AS reversedDouble FROM reverse;
```

dari query tersebut akan menghasilkan tabel berikut yang menunjukan bahwa function reverse berhasilmembalikan value dari tiap field yang ada, baik itu string, integer, long maupun double, sesuai dengan rencana awal.

![image](https://github.com/ferdyansahalfariz/belajar-linux/assets/96871156/78c386aa-07bc-4eaf-8d6a-2dbd2be945cc)

# 19 Juni 2024
Pada kesempatan kali ini, saya akan mencoba untuk mengkombinasikan penggunaan dari ksql dan kafka connect dengan melakukan agregasi score di ksql dengan penerapan window tumbling, data yang diinput memiliki jumlah besar yaitu sebanyak 1 juta data dengan jumlah partisi yaitu 3. data yang sudah berhasil diinput kemudian juga akan dilakukan sink ke database postgresql dengan menambahkan kolom kafka offset, timestamp serta partisinya.

1. Langkah pertama yaitu membuat stream baru yang juga secara otomatis membuat topic baru dengan 3 partisi menggunakan perintah:

```
CREATE STREAM match_window (matchId VARCHAR, team VARCHAR, score INT, timestamp STRING) WITH (kafka_topic='matches_window', value_format='AVRO', partitions=3, TIMESTAMP='timestamp', TIMESTAMP_FORMAT='yyyy-MM-dd''T''HH:mm:ssXXX');
```

2. setelah stream berhasil dibuat, coba lakukan window tumbling dengan perintah berikut:

```
SELECT team,
       SUM(score) AS total_score,
       WINDOWSTART AS window_start,
       WINDOWEND AS window_end
FROM match_window
WINDOW TUMBLING (SIZE 1 HOUR)
GROUP BY team
EMIT CHANGES;
```

Adapun penjelasannya yaitu query tersebut akan menampilkan update secara realtime untuk total skor yang dimiliki oleh setiap tim pada satuan waktu tertentu yang ditentukan dalam jendela atau window yang diatur, dalam hal ini saya akan seimbangkan dengan dataset dummynya menjadi 1 jam, query tersebut juga akan mengelompokkannya berdasarkan tim.

3. Sambil query tersebut berjalan, maka saya akan produce data ke stream tersebut sejumlah 1 juta data dengan contoh query berikut:

```
INSERT INTO match_window (matchId, team, score, timestamp) VALUES ('match1', 'barca', 3, '2024-06-19T09:00:00+07:00');
INSERT INTO match_window (matchId, team, score, timestamp) VALUES ('match1', 'madrid', 1, '2024-06-19T09:00:00+07:00');
INSERT INTO match_window (matchId, team, score, timestamp) VALUES ('match2', 'valencia', 3, '2024-06-19T10:00:00+07:00');
INSERT INTO match_window (matchId, team, score, timestamp) VALUES ('match2', 'barca', 2, '2024-06-19T10:00:00+07:00');
INSERT INTO match_window (matchId, team, score, timestamp) VALUES ('match3', 'madrid', 5, '2024-06-19T11:00:00+07:00');
INSERT INTO match_window (matchId, team, score, timestamp) VALUES ('match3', 'valencia', 2, '2024-06-19T11:00:00+07:00');
INSERT INTO match_window (matchId, team, score, timestamp) VALUES ('match4', 'barca', 5, '2024-06-19T12:00:00+07:00');
INSERT INTO match_window (matchId, team, score, timestamp) VALUES ('match4', 'valencia', 0, '2024-06-19T12:00:00+07:00');
INSERT INTO match_window (matchId, team, score, timestamp) VALUES ('match5', 'madrid', 0, '2024-06-19T13:00:00+07:00');
INSERT INTO match_window (matchId, team, score, timestamp) VALUES ('match5', 'valencia', 5, '2024-06-19T13:00:00+07:00');
```

Untuk memudahkan produce 1 juta data, saya akan menggunakan script shell untuk memanggil ksql dan running 10 query insert diatas sebanyak 100 ribu kali sehingga data yang dihasilkan akan berjumlah 1 juta. Buat filenya dan jalankan dengan isi scriptnya sebagai berikut:

```
#!/bin/bash

# URL KSQL server
KSQL_SERVER="http://localhost:8088"

# KSQL command
KSQL_COMMAND="
INSERT INTO match_window (matchId, team, score, timestamp) VALUES ('match1', 'barca', 3, '2024-06-19T09:00:00+07:00');
INSERT INTO match_window (matchId, team, score, timestamp) VALUES ('match1', 'madrid', 1, '2024-06-19T09:00:00+07:00');
INSERT INTO match_window (matchId, team, score, timestamp) VALUES ('match2', 'valencia', 3, '2024-06-19T10:00:00+07:00');
INSERT INTO match_window (matchId, team, score, timestamp) VALUES ('match2', 'barca', 2, '2024-06-19T10:00:00+07:00');
INSERT INTO match_window (matchId, team, score, timestamp) VALUES ('match3', 'madrid', 5, '2024-06-19T11:00:00+07:00');
INSERT INTO match_window (matchId, team, score, timestamp) VALUES ('match3', 'valencia', 2, '2024-06-19T11:00:00+07:00');
INSERT INTO match_window (matchId, team, score, timestamp) VALUES ('match4', 'barca', 5, '2024-06-19T12:00:00+07:00');
INSERT INTO match_window (matchId, team, score, timestamp) VALUES ('match4', 'valencia', 0, '2024-06-19T12:00:00+07:00');
INSERT INTO match_window (matchId, team, score, timestamp) VALUES ('match5', 'madrid', 0, '2024-06-19T13:00:00+07:00');
INSERT INTO match_window (matchId, team, score, timestamp) VALUES ('match5', 'valencia', 5, '2024-06-19T13:00:00+07:00');
"

# Function to send KSQL command
send_ksql_command() {
  curl -s -X POST "${KSQL_SERVER}/ksql" \
    -H "Content-Type: application/vnd.ksql.v1+json; charset=utf-8" \
    -d @- <<EOF
{
  "ksql": "$KSQL_COMMAND",
  "streamsProperties": {}
}
EOF
}

# Loop to run the command 100,000 times
for i in {1..100000}; do
  response=$(send_ksql_command)

  # Check the response status and log success or error
  if [[ $response == *"SUCCESS"* ]]; then
    echo "$i sukses"
  else
    echo "$i error"
  fi

  # Optional: Add a small delay to avoid overwhelming the server
  sleep 0.01
done

echo "Selesai mengirim 100,000 records ke KSQL"
```

script tersebut akan secara otomatis melakukan insert menggunakan curl ke server tempat ksql berjalan dan akan secara otomatis berhenti saat seluruh iterasi pada data sudah berhasil di tercapai.

4. query window dapat di cek saat script berjalan dengan selalu update secara realtime berdasarkan jumlah score yang dicetak oleh tim tertentu pada satu satuan waktu tertentu.

![image](https://github.com/ferdyansahalfariz/belajar-linux/assets/96871156/5b48fd46-e58a-4ad5-bd89-aa02303a439e)

![image](https://github.com/ferdyansahalfariz/belajar-linux/assets/96871156/128af90a-a35a-4a2d-a243-7c19b39def28)

5. Setelah semua data berhasil di produce, lakukan sink dengan membuat database pada server postgresql, buat file konfigurasinya, dan import ke C3 pada bagian connect. Berikut konfigurasinya:

```
{
  "name": "postgres-window-sink-connector",
  "config": {
    "connector.class": "io.confluent.connect.jdbc.JdbcSinkConnector",
    "tasks.max": "2",
    "key.converter": "io.confluent.connect.avro.AvroConverter",
    "value.converter": "io.confluent.connect.avro.AvroConverter",
    "transforms": "InsertField",
    "topics": "matches_window",
    "transforms.InsertField.type": "org.apache.kafka.connect.transforms.InsertField$Value",
    "transforms.InsertField.offset.field": "kafka_offset",
    "transforms.InsertField.timestamp.field": "kafka_timestamp",
    "transforms.InsertField.partition.field": "kafka_partition",
    "connection.url": "jdbc:postgresql://10.100.13.24:5432/windowtumbling",
    "connection.user": "ferdy",
    "connection.password": "ferdy",
    "dialect.name": "PostgreSqlDatabaseDialect",
    "insert.mode": "insert",
    "table.name.format": "public.matches_window",
    "pk.mode": "none",
    "key.converter.schema.registry.url": "http://master.k8s.alldataint.com:8081",
    "value.converter.schema.registry.url": "http://master.k8s.alldataint.com:8081",
    "auto.create": "true",
    "auto.evolve": "true"
  }
}
```

6. Buka dbeaver untuk melihat apakah semua data sudah berhasil sink ke database postgresql dengan menyambungkannya ke database yang sebelumnya sudah disambungkan lalu gunakan perintah select * from matches_window; untuk melihat datanya. berikut adalah hasilnya:



